name: "Darkent2Caffe"
input: "data"
input_shape {
    dim: 1
    dim: 3
    dim: 416
    dim: 416
}
layer {
    bottom: "data"
    top: "layer1-conv"
    name: "layer1-conv"
    type: "Convolution"
    convolution_param {
        num_output: 32
        kernel_size: 3
        pad: 1
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer1-conv"
    top: "layer1-conv"
    name: "layer1-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer1-conv"
    top: "layer1-conv"
    name: "layer1-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer1-conv"
    top: "layer1-conv"
    name: "layer1-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer1-conv"
    top: "layer2-conv"
    name: "layer2-conv"
    type: "Convolution"
    convolution_param {
        num_output: 64
        kernel_size: 3
        pad: 1
        stride: 2
        bias_term: false
    }
}
layer {
    bottom: "layer2-conv"
    top: "layer2-conv"
    name: "layer2-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer2-conv"
    top: "layer2-conv"
    name: "layer2-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer2-conv"
    top: "layer2-conv"
    name: "layer2-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer2-conv"
    top: "layer3-conv"
    name: "layer3-conv"
    type: "Convolution"
    convolution_param {
        num_output: 32
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer3-conv"
    top: "layer3-conv"
    name: "layer3-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer3-conv"
    top: "layer3-conv"
    name: "layer3-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer3-conv"
    top: "layer3-conv"
    name: "layer3-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer3-conv"
    top: "layer4-conv"
    name: "layer4-conv"
    type: "Convolution"
    convolution_param {
        num_output: 64
        kernel_size: 3
        pad: 1
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer4-conv"
    top: "layer4-conv"
    name: "layer4-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer4-conv"
    top: "layer4-conv"
    name: "layer4-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer4-conv"
    top: "layer4-conv"
    name: "layer4-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer2-conv"
    bottom: "layer4-conv"
    top: "layer5-shortcut"
    name: "layer5-shortcut"
    type: "Eltwise"
    eltwise_param {
        operation: SUM
    }
}
layer {
    bottom: "layer5-shortcut"
    top: "layer6-conv"
    name: "layer6-conv"
    type: "Convolution"
    convolution_param {
        num_output: 128
        kernel_size: 3
        pad: 1
        stride: 2
        bias_term: false
    }
}
layer {
    bottom: "layer6-conv"
    top: "layer6-conv"
    name: "layer6-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer6-conv"
    top: "layer6-conv"
    name: "layer6-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer6-conv"
    top: "layer6-conv"
    name: "layer6-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer6-conv"
    top: "layer7-conv"
    name: "layer7-conv"
    type: "Convolution"
    convolution_param {
        num_output: 64
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer7-conv"
    top: "layer7-conv"
    name: "layer7-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer7-conv"
    top: "layer7-conv"
    name: "layer7-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer7-conv"
    top: "layer7-conv"
    name: "layer7-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer7-conv"
    top: "layer8-conv"
    name: "layer8-conv"
    type: "Convolution"
    convolution_param {
        num_output: 128
        kernel_size: 3
        pad: 1
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer8-conv"
    top: "layer8-conv"
    name: "layer8-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer8-conv"
    top: "layer8-conv"
    name: "layer8-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer8-conv"
    top: "layer8-conv"
    name: "layer8-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer6-conv"
    bottom: "layer8-conv"
    top: "layer9-shortcut"
    name: "layer9-shortcut"
    type: "Eltwise"
    eltwise_param {
        operation: SUM
    }
}
layer {
    bottom: "layer9-shortcut"
    top: "layer10-conv"
    name: "layer10-conv"
    type: "Convolution"
    convolution_param {
        num_output: 64
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer10-conv"
    top: "layer10-conv"
    name: "layer10-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer10-conv"
    top: "layer10-conv"
    name: "layer10-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer10-conv"
    top: "layer10-conv"
    name: "layer10-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer10-conv"
    top: "layer11-conv"
    name: "layer11-conv"
    type: "Convolution"
    convolution_param {
        num_output: 128
        kernel_size: 3
        pad: 1
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer11-conv"
    top: "layer11-conv"
    name: "layer11-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer11-conv"
    top: "layer11-conv"
    name: "layer11-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer11-conv"
    top: "layer11-conv"
    name: "layer11-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer9-shortcut"
    bottom: "layer11-conv"
    top: "layer12-shortcut"
    name: "layer12-shortcut"
    type: "Eltwise"
    eltwise_param {
        operation: SUM
    }
}
layer {
    bottom: "layer12-shortcut"
    top: "layer13-conv"
    name: "layer13-conv"
    type: "Convolution"
    convolution_param {
        num_output: 256
        kernel_size: 3
        pad: 1
        stride: 2
        bias_term: false
    }
}
layer {
    bottom: "layer13-conv"
    top: "layer13-conv"
    name: "layer13-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer13-conv"
    top: "layer13-conv"
    name: "layer13-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer13-conv"
    top: "layer13-conv"
    name: "layer13-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer13-conv"
    top: "layer14-conv"
    name: "layer14-conv"
    type: "Convolution"
    convolution_param {
        num_output: 128
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer14-conv"
    top: "layer14-conv"
    name: "layer14-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer14-conv"
    top: "layer14-conv"
    name: "layer14-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer14-conv"
    top: "layer14-conv"
    name: "layer14-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer14-conv"
    top: "layer15-conv"
    name: "layer15-conv"
    type: "Convolution"
    convolution_param {
        num_output: 256
        kernel_size: 3
        pad: 1
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer15-conv"
    top: "layer15-conv"
    name: "layer15-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer15-conv"
    top: "layer15-conv"
    name: "layer15-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer15-conv"
    top: "layer15-conv"
    name: "layer15-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer13-conv"
    bottom: "layer15-conv"
    top: "layer16-shortcut"
    name: "layer16-shortcut"
    type: "Eltwise"
    eltwise_param {
        operation: SUM
    }
}
layer {
    bottom: "layer16-shortcut"
    top: "layer17-conv"
    name: "layer17-conv"
    type: "Convolution"
    convolution_param {
        num_output: 128
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer17-conv"
    top: "layer17-conv"
    name: "layer17-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer17-conv"
    top: "layer17-conv"
    name: "layer17-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer17-conv"
    top: "layer17-conv"
    name: "layer17-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer17-conv"
    top: "layer18-conv"
    name: "layer18-conv"
    type: "Convolution"
    convolution_param {
        num_output: 256
        kernel_size: 3
        pad: 1
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer18-conv"
    top: "layer18-conv"
    name: "layer18-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer18-conv"
    top: "layer18-conv"
    name: "layer18-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer18-conv"
    top: "layer18-conv"
    name: "layer18-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer16-shortcut"
    bottom: "layer18-conv"
    top: "layer19-shortcut"
    name: "layer19-shortcut"
    type: "Eltwise"
    eltwise_param {
        operation: SUM
    }
}
layer {
    bottom: "layer19-shortcut"
    top: "layer20-conv"
    name: "layer20-conv"
    type: "Convolution"
    convolution_param {
        num_output: 128
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer20-conv"
    top: "layer20-conv"
    name: "layer20-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer20-conv"
    top: "layer20-conv"
    name: "layer20-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer20-conv"
    top: "layer20-conv"
    name: "layer20-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer20-conv"
    top: "layer21-conv"
    name: "layer21-conv"
    type: "Convolution"
    convolution_param {
        num_output: 256
        kernel_size: 3
        pad: 1
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer21-conv"
    top: "layer21-conv"
    name: "layer21-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer21-conv"
    top: "layer21-conv"
    name: "layer21-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer21-conv"
    top: "layer21-conv"
    name: "layer21-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer19-shortcut"
    bottom: "layer21-conv"
    top: "layer22-shortcut"
    name: "layer22-shortcut"
    type: "Eltwise"
    eltwise_param {
        operation: SUM
    }
}
layer {
    bottom: "layer22-shortcut"
    top: "layer23-conv"
    name: "layer23-conv"
    type: "Convolution"
    convolution_param {
        num_output: 128
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer23-conv"
    top: "layer23-conv"
    name: "layer23-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer23-conv"
    top: "layer23-conv"
    name: "layer23-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer23-conv"
    top: "layer23-conv"
    name: "layer23-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer23-conv"
    top: "layer24-conv"
    name: "layer24-conv"
    type: "Convolution"
    convolution_param {
        num_output: 256
        kernel_size: 3
        pad: 1
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer24-conv"
    top: "layer24-conv"
    name: "layer24-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer24-conv"
    top: "layer24-conv"
    name: "layer24-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer24-conv"
    top: "layer24-conv"
    name: "layer24-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer22-shortcut"
    bottom: "layer24-conv"
    top: "layer25-shortcut"
    name: "layer25-shortcut"
    type: "Eltwise"
    eltwise_param {
        operation: SUM
    }
}
layer {
    bottom: "layer25-shortcut"
    top: "layer26-conv"
    name: "layer26-conv"
    type: "Convolution"
    convolution_param {
        num_output: 128
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer26-conv"
    top: "layer26-conv"
    name: "layer26-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer26-conv"
    top: "layer26-conv"
    name: "layer26-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer26-conv"
    top: "layer26-conv"
    name: "layer26-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer26-conv"
    top: "layer27-conv"
    name: "layer27-conv"
    type: "Convolution"
    convolution_param {
        num_output: 256
        kernel_size: 3
        pad: 1
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer27-conv"
    top: "layer27-conv"
    name: "layer27-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer27-conv"
    top: "layer27-conv"
    name: "layer27-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer27-conv"
    top: "layer27-conv"
    name: "layer27-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer25-shortcut"
    bottom: "layer27-conv"
    top: "layer28-shortcut"
    name: "layer28-shortcut"
    type: "Eltwise"
    eltwise_param {
        operation: SUM
    }
}
layer {
    bottom: "layer28-shortcut"
    top: "layer29-conv"
    name: "layer29-conv"
    type: "Convolution"
    convolution_param {
        num_output: 128
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer29-conv"
    top: "layer29-conv"
    name: "layer29-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer29-conv"
    top: "layer29-conv"
    name: "layer29-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer29-conv"
    top: "layer29-conv"
    name: "layer29-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer29-conv"
    top: "layer30-conv"
    name: "layer30-conv"
    type: "Convolution"
    convolution_param {
        num_output: 256
        kernel_size: 3
        pad: 1
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer30-conv"
    top: "layer30-conv"
    name: "layer30-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer30-conv"
    top: "layer30-conv"
    name: "layer30-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer30-conv"
    top: "layer30-conv"
    name: "layer30-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer28-shortcut"
    bottom: "layer30-conv"
    top: "layer31-shortcut"
    name: "layer31-shortcut"
    type: "Eltwise"
    eltwise_param {
        operation: SUM
    }
}
layer {
    bottom: "layer31-shortcut"
    top: "layer32-conv"
    name: "layer32-conv"
    type: "Convolution"
    convolution_param {
        num_output: 128
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer32-conv"
    top: "layer32-conv"
    name: "layer32-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer32-conv"
    top: "layer32-conv"
    name: "layer32-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer32-conv"
    top: "layer32-conv"
    name: "layer32-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer32-conv"
    top: "layer33-conv"
    name: "layer33-conv"
    type: "Convolution"
    convolution_param {
        num_output: 256
        kernel_size: 3
        pad: 1
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer33-conv"
    top: "layer33-conv"
    name: "layer33-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer33-conv"
    top: "layer33-conv"
    name: "layer33-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer33-conv"
    top: "layer33-conv"
    name: "layer33-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer31-shortcut"
    bottom: "layer33-conv"
    top: "layer34-shortcut"
    name: "layer34-shortcut"
    type: "Eltwise"
    eltwise_param {
        operation: SUM
    }
}
layer {
    bottom: "layer34-shortcut"
    top: "layer35-conv"
    name: "layer35-conv"
    type: "Convolution"
    convolution_param {
        num_output: 128
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer35-conv"
    top: "layer35-conv"
    name: "layer35-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer35-conv"
    top: "layer35-conv"
    name: "layer35-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer35-conv"
    top: "layer35-conv"
    name: "layer35-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer35-conv"
    top: "layer36-conv"
    name: "layer36-conv"
    type: "Convolution"
    convolution_param {
        num_output: 256
        kernel_size: 3
        pad: 1
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer36-conv"
    top: "layer36-conv"
    name: "layer36-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer36-conv"
    top: "layer36-conv"
    name: "layer36-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer36-conv"
    top: "layer36-conv"
    name: "layer36-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer34-shortcut"
    bottom: "layer36-conv"
    top: "layer37-shortcut"
    name: "layer37-shortcut"
    type: "Eltwise"
    eltwise_param {
        operation: SUM
    }
}
layer {
    bottom: "layer37-shortcut"
    top: "layer38-conv"
    name: "layer38-conv"
    type: "Convolution"
    convolution_param {
        num_output: 512
        kernel_size: 3
        pad: 1
        stride: 2
        bias_term: false
    }
}
layer {
    bottom: "layer38-conv"
    top: "layer38-conv"
    name: "layer38-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer38-conv"
    top: "layer38-conv"
    name: "layer38-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer38-conv"
    top: "layer38-conv"
    name: "layer38-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer38-conv"
    top: "layer39-conv"
    name: "layer39-conv"
    type: "Convolution"
    convolution_param {
        num_output: 256
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer39-conv"
    top: "layer39-conv"
    name: "layer39-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer39-conv"
    top: "layer39-conv"
    name: "layer39-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer39-conv"
    top: "layer39-conv"
    name: "layer39-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer39-conv"
    top: "layer40-conv"
    name: "layer40-conv"
    type: "Convolution"
    convolution_param {
        num_output: 512
        kernel_size: 3
        pad: 1
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer40-conv"
    top: "layer40-conv"
    name: "layer40-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer40-conv"
    top: "layer40-conv"
    name: "layer40-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer40-conv"
    top: "layer40-conv"
    name: "layer40-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer38-conv"
    bottom: "layer40-conv"
    top: "layer41-shortcut"
    name: "layer41-shortcut"
    type: "Eltwise"
    eltwise_param {
        operation: SUM
    }
}
layer {
    bottom: "layer41-shortcut"
    top: "layer42-conv"
    name: "layer42-conv"
    type: "Convolution"
    convolution_param {
        num_output: 256
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer42-conv"
    top: "layer42-conv"
    name: "layer42-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer42-conv"
    top: "layer42-conv"
    name: "layer42-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer42-conv"
    top: "layer42-conv"
    name: "layer42-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer42-conv"
    top: "layer43-conv"
    name: "layer43-conv"
    type: "Convolution"
    convolution_param {
        num_output: 512
        kernel_size: 3
        pad: 1
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer43-conv"
    top: "layer43-conv"
    name: "layer43-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer43-conv"
    top: "layer43-conv"
    name: "layer43-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer43-conv"
    top: "layer43-conv"
    name: "layer43-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer41-shortcut"
    bottom: "layer43-conv"
    top: "layer44-shortcut"
    name: "layer44-shortcut"
    type: "Eltwise"
    eltwise_param {
        operation: SUM
    }
}
layer {
    bottom: "layer44-shortcut"
    top: "layer45-conv"
    name: "layer45-conv"
    type: "Convolution"
    convolution_param {
        num_output: 256
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer45-conv"
    top: "layer45-conv"
    name: "layer45-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer45-conv"
    top: "layer45-conv"
    name: "layer45-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer45-conv"
    top: "layer45-conv"
    name: "layer45-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer45-conv"
    top: "layer46-conv"
    name: "layer46-conv"
    type: "Convolution"
    convolution_param {
        num_output: 512
        kernel_size: 3
        pad: 1
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer46-conv"
    top: "layer46-conv"
    name: "layer46-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer46-conv"
    top: "layer46-conv"
    name: "layer46-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer46-conv"
    top: "layer46-conv"
    name: "layer46-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer44-shortcut"
    bottom: "layer46-conv"
    top: "layer47-shortcut"
    name: "layer47-shortcut"
    type: "Eltwise"
    eltwise_param {
        operation: SUM
    }
}
layer {
    bottom: "layer47-shortcut"
    top: "layer48-conv"
    name: "layer48-conv"
    type: "Convolution"
    convolution_param {
        num_output: 256
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer48-conv"
    top: "layer48-conv"
    name: "layer48-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer48-conv"
    top: "layer48-conv"
    name: "layer48-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer48-conv"
    top: "layer48-conv"
    name: "layer48-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer48-conv"
    top: "layer49-conv"
    name: "layer49-conv"
    type: "Convolution"
    convolution_param {
        num_output: 512
        kernel_size: 3
        pad: 1
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer49-conv"
    top: "layer49-conv"
    name: "layer49-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer49-conv"
    top: "layer49-conv"
    name: "layer49-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer49-conv"
    top: "layer49-conv"
    name: "layer49-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer47-shortcut"
    bottom: "layer49-conv"
    top: "layer50-shortcut"
    name: "layer50-shortcut"
    type: "Eltwise"
    eltwise_param {
        operation: SUM
    }
}
layer {
    bottom: "layer50-shortcut"
    top: "layer51-conv"
    name: "layer51-conv"
    type: "Convolution"
    convolution_param {
        num_output: 256
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer51-conv"
    top: "layer51-conv"
    name: "layer51-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer51-conv"
    top: "layer51-conv"
    name: "layer51-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer51-conv"
    top: "layer51-conv"
    name: "layer51-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer51-conv"
    top: "layer52-conv"
    name: "layer52-conv"
    type: "Convolution"
    convolution_param {
        num_output: 512
        kernel_size: 3
        pad: 1
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer52-conv"
    top: "layer52-conv"
    name: "layer52-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer52-conv"
    top: "layer52-conv"
    name: "layer52-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer52-conv"
    top: "layer52-conv"
    name: "layer52-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer50-shortcut"
    bottom: "layer52-conv"
    top: "layer53-shortcut"
    name: "layer53-shortcut"
    type: "Eltwise"
    eltwise_param {
        operation: SUM
    }
}
layer {
    bottom: "layer53-shortcut"
    top: "layer54-conv"
    name: "layer54-conv"
    type: "Convolution"
    convolution_param {
        num_output: 256
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer54-conv"
    top: "layer54-conv"
    name: "layer54-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer54-conv"
    top: "layer54-conv"
    name: "layer54-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer54-conv"
    top: "layer54-conv"
    name: "layer54-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer54-conv"
    top: "layer55-conv"
    name: "layer55-conv"
    type: "Convolution"
    convolution_param {
        num_output: 512
        kernel_size: 3
        pad: 1
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer55-conv"
    top: "layer55-conv"
    name: "layer55-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer55-conv"
    top: "layer55-conv"
    name: "layer55-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer55-conv"
    top: "layer55-conv"
    name: "layer55-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer53-shortcut"
    bottom: "layer55-conv"
    top: "layer56-shortcut"
    name: "layer56-shortcut"
    type: "Eltwise"
    eltwise_param {
        operation: SUM
    }
}
layer {
    bottom: "layer56-shortcut"
    top: "layer57-conv"
    name: "layer57-conv"
    type: "Convolution"
    convolution_param {
        num_output: 256
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer57-conv"
    top: "layer57-conv"
    name: "layer57-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer57-conv"
    top: "layer57-conv"
    name: "layer57-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer57-conv"
    top: "layer57-conv"
    name: "layer57-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer57-conv"
    top: "layer58-conv"
    name: "layer58-conv"
    type: "Convolution"
    convolution_param {
        num_output: 512
        kernel_size: 3
        pad: 1
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer58-conv"
    top: "layer58-conv"
    name: "layer58-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer58-conv"
    top: "layer58-conv"
    name: "layer58-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer58-conv"
    top: "layer58-conv"
    name: "layer58-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer56-shortcut"
    bottom: "layer58-conv"
    top: "layer59-shortcut"
    name: "layer59-shortcut"
    type: "Eltwise"
    eltwise_param {
        operation: SUM
    }
}
layer {
    bottom: "layer59-shortcut"
    top: "layer60-conv"
    name: "layer60-conv"
    type: "Convolution"
    convolution_param {
        num_output: 256
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer60-conv"
    top: "layer60-conv"
    name: "layer60-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer60-conv"
    top: "layer60-conv"
    name: "layer60-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer60-conv"
    top: "layer60-conv"
    name: "layer60-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer60-conv"
    top: "layer61-conv"
    name: "layer61-conv"
    type: "Convolution"
    convolution_param {
        num_output: 512
        kernel_size: 3
        pad: 1
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer61-conv"
    top: "layer61-conv"
    name: "layer61-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer61-conv"
    top: "layer61-conv"
    name: "layer61-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer61-conv"
    top: "layer61-conv"
    name: "layer61-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer59-shortcut"
    bottom: "layer61-conv"
    top: "layer62-shortcut"
    name: "layer62-shortcut"
    type: "Eltwise"
    eltwise_param {
        operation: SUM
    }
}
layer {
    bottom: "layer62-shortcut"
    top: "layer63-conv"
    name: "layer63-conv"
    type: "Convolution"
    convolution_param {
        num_output: 1024
        kernel_size: 3
        pad: 1
        stride: 2
        bias_term: false
    }
}
layer {
    bottom: "layer63-conv"
    top: "layer63-conv"
    name: "layer63-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer63-conv"
    top: "layer63-conv"
    name: "layer63-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer63-conv"
    top: "layer63-conv"
    name: "layer63-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer63-conv"
    top: "layer64-conv"
    name: "layer64-conv"
    type: "Convolution"
    convolution_param {
        num_output: 512
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer64-conv"
    top: "layer64-conv"
    name: "layer64-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer64-conv"
    top: "layer64-conv"
    name: "layer64-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer64-conv"
    top: "layer64-conv"
    name: "layer64-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer64-conv"
    top: "layer65-conv"
    name: "layer65-conv"
    type: "Convolution"
    convolution_param {
        num_output: 1024
        kernel_size: 3
        pad: 1
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer65-conv"
    top: "layer65-conv"
    name: "layer65-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer65-conv"
    top: "layer65-conv"
    name: "layer65-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer65-conv"
    top: "layer65-conv"
    name: "layer65-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer63-conv"
    bottom: "layer65-conv"
    top: "layer66-shortcut"
    name: "layer66-shortcut"
    type: "Eltwise"
    eltwise_param {
        operation: SUM
    }
}
layer {
    bottom: "layer66-shortcut"
    top: "layer67-conv"
    name: "layer67-conv"
    type: "Convolution"
    convolution_param {
        num_output: 512
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer67-conv"
    top: "layer67-conv"
    name: "layer67-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer67-conv"
    top: "layer67-conv"
    name: "layer67-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer67-conv"
    top: "layer67-conv"
    name: "layer67-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer67-conv"
    top: "layer68-conv"
    name: "layer68-conv"
    type: "Convolution"
    convolution_param {
        num_output: 1024
        kernel_size: 3
        pad: 1
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer68-conv"
    top: "layer68-conv"
    name: "layer68-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer68-conv"
    top: "layer68-conv"
    name: "layer68-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer68-conv"
    top: "layer68-conv"
    name: "layer68-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer66-shortcut"
    bottom: "layer68-conv"
    top: "layer69-shortcut"
    name: "layer69-shortcut"
    type: "Eltwise"
    eltwise_param {
        operation: SUM
    }
}
layer {
    bottom: "layer69-shortcut"
    top: "layer70-conv"
    name: "layer70-conv"
    type: "Convolution"
    convolution_param {
        num_output: 512
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer70-conv"
    top: "layer70-conv"
    name: "layer70-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer70-conv"
    top: "layer70-conv"
    name: "layer70-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer70-conv"
    top: "layer70-conv"
    name: "layer70-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer70-conv"
    top: "layer71-conv"
    name: "layer71-conv"
    type: "Convolution"
    convolution_param {
        num_output: 1024
        kernel_size: 3
        pad: 1
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer71-conv"
    top: "layer71-conv"
    name: "layer71-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer71-conv"
    top: "layer71-conv"
    name: "layer71-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer71-conv"
    top: "layer71-conv"
    name: "layer71-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer69-shortcut"
    bottom: "layer71-conv"
    top: "layer72-shortcut"
    name: "layer72-shortcut"
    type: "Eltwise"
    eltwise_param {
        operation: SUM
    }
}
layer {
    bottom: "layer72-shortcut"
    top: "layer73-conv"
    name: "layer73-conv"
    type: "Convolution"
    convolution_param {
        num_output: 512
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer73-conv"
    top: "layer73-conv"
    name: "layer73-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer73-conv"
    top: "layer73-conv"
    name: "layer73-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer73-conv"
    top: "layer73-conv"
    name: "layer73-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer73-conv"
    top: "layer74-conv"
    name: "layer74-conv"
    type: "Convolution"
    convolution_param {
        num_output: 1024
        kernel_size: 3
        pad: 1
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer74-conv"
    top: "layer74-conv"
    name: "layer74-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer74-conv"
    top: "layer74-conv"
    name: "layer74-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer74-conv"
    top: "layer74-conv"
    name: "layer74-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer72-shortcut"
    bottom: "layer74-conv"
    top: "layer75-shortcut"
    name: "layer75-shortcut"
    type: "Eltwise"
    eltwise_param {
        operation: SUM
    }
}
layer {
    bottom: "layer75-shortcut"
    top: "layer76-conv"
    name: "layer76-conv"
    type: "Convolution"
    convolution_param {
        num_output: 512
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer76-conv"
    top: "layer76-conv"
    name: "layer76-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer76-conv"
    top: "layer76-conv"
    name: "layer76-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer76-conv"
    top: "layer76-conv"
    name: "layer76-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer76-conv"
    top: "layer77-conv"
    name: "layer77-conv"
    type: "Convolution"
    convolution_param {
        num_output: 1024
        kernel_size: 3
        pad: 1
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer77-conv"
    top: "layer77-conv"
    name: "layer77-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer77-conv"
    top: "layer77-conv"
    name: "layer77-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer77-conv"
    top: "layer77-conv"
    name: "layer77-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer77-conv"
    top: "layer78-conv"
    name: "layer78-conv"
    type: "Convolution"
    convolution_param {
        num_output: 512
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer78-conv"
    top: "layer78-conv"
    name: "layer78-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer78-conv"
    top: "layer78-conv"
    name: "layer78-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer78-conv"
    top: "layer78-conv"
    name: "layer78-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer78-conv"
    top: "layer79-conv"
    name: "layer79-conv"
    type: "Convolution"
    convolution_param {
        num_output: 1024
        kernel_size: 3
        pad: 1
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer79-conv"
    top: "layer79-conv"
    name: "layer79-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer79-conv"
    top: "layer79-conv"
    name: "layer79-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer79-conv"
    top: "layer79-conv"
    name: "layer79-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer79-conv"
    top: "layer80-conv"
    name: "layer80-conv"
    type: "Convolution"
    convolution_param {
        num_output: 512
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer80-conv"
    top: "layer80-conv"
    name: "layer80-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer80-conv"
    top: "layer80-conv"
    name: "layer80-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer80-conv"
    top: "layer80-conv"
    name: "layer80-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer80-conv"
    top: "layer81-conv"
    name: "layer81-conv"
    type: "Convolution"
    convolution_param {
        num_output: 1024
        kernel_size: 3
        pad: 1
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer81-conv"
    top: "layer81-conv"
    name: "layer81-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer81-conv"
    top: "layer81-conv"
    name: "layer81-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer81-conv"
    top: "layer81-conv"
    name: "layer81-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer81-conv"
    top: "layer82-conv"
    name: "layer82-conv"
    type: "Convolution"
    convolution_param {
        num_output: 255
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: true
    }
}
layer {
    bottom: "layer80-conv"
    top: "layer85-conv"
    name: "layer85-conv"
    type: "Convolution"
    convolution_param {
        num_output: 256
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer85-conv"
    top: "layer85-conv"
    name: "layer85-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer85-conv"
    top: "layer85-conv"
    name: "layer85-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer85-conv"
    top: "layer85-conv"
    name: "layer85-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer85-conv"
    top: "layer86-upsample"
    name: "layer86-upsample"
    type: "Upsample"
    upsample_param {
        scale: 2
    }
}
layer {
    bottom: "layer86-upsample"
    bottom: "layer62-shortcut"
    top: "layer87-concat"
    name: "layer87-concat"
    type: "Concat"
}
layer {
    bottom: "layer87-concat"
    top: "layer88-conv"
    name: "layer88-conv"
    type: "Convolution"
    convolution_param {
        num_output: 256
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer88-conv"
    top: "layer88-conv"
    name: "layer88-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer88-conv"
    top: "layer88-conv"
    name: "layer88-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer88-conv"
    top: "layer88-conv"
    name: "layer88-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer88-conv"
    top: "layer89-conv"
    name: "layer89-conv"
    type: "Convolution"
    convolution_param {
        num_output: 512
        kernel_size: 3
        pad: 1
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer89-conv"
    top: "layer89-conv"
    name: "layer89-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer89-conv"
    top: "layer89-conv"
    name: "layer89-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer89-conv"
    top: "layer89-conv"
    name: "layer89-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer89-conv"
    top: "layer90-conv"
    name: "layer90-conv"
    type: "Convolution"
    convolution_param {
        num_output: 256
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer90-conv"
    top: "layer90-conv"
    name: "layer90-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer90-conv"
    top: "layer90-conv"
    name: "layer90-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer90-conv"
    top: "layer90-conv"
    name: "layer90-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer90-conv"
    top: "layer91-conv"
    name: "layer91-conv"
    type: "Convolution"
    convolution_param {
        num_output: 512
        kernel_size: 3
        pad: 1
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer91-conv"
    top: "layer91-conv"
    name: "layer91-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer91-conv"
    top: "layer91-conv"
    name: "layer91-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer91-conv"
    top: "layer91-conv"
    name: "layer91-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer91-conv"
    top: "layer92-conv"
    name: "layer92-conv"
    type: "Convolution"
    convolution_param {
        num_output: 256
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer92-conv"
    top: "layer92-conv"
    name: "layer92-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer92-conv"
    top: "layer92-conv"
    name: "layer92-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer92-conv"
    top: "layer92-conv"
    name: "layer92-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer92-conv"
    top: "layer93-conv"
    name: "layer93-conv"
    type: "Convolution"
    convolution_param {
        num_output: 512
        kernel_size: 3
        pad: 1
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer93-conv"
    top: "layer93-conv"
    name: "layer93-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer93-conv"
    top: "layer93-conv"
    name: "layer93-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer93-conv"
    top: "layer93-conv"
    name: "layer93-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer93-conv"
    top: "layer94-conv"
    name: "layer94-conv"
    type: "Convolution"
    convolution_param {
        num_output: 255
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: true
    }
}
layer {
    bottom: "layer92-conv"
    top: "layer97-conv"
    name: "layer97-conv"
    type: "Convolution"
    convolution_param {
        num_output: 128
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer97-conv"
    top: "layer97-conv"
    name: "layer97-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer97-conv"
    top: "layer97-conv"
    name: "layer97-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer97-conv"
    top: "layer97-conv"
    name: "layer97-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer97-conv"
    top: "layer98-upsample"
    name: "layer98-upsample"
    type: "Upsample"
    upsample_param {
        scale: 2
    }
}
layer {
    bottom: "layer98-upsample"
    bottom: "layer37-shortcut"
    top: "layer99-concat"
    name: "layer99-concat"
    type: "Concat"
}
layer {
    bottom: "layer99-concat"
    top: "layer100-conv"
    name: "layer100-conv"
    type: "Convolution"
    convolution_param {
        num_output: 128
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer100-conv"
    top: "layer100-conv"
    name: "layer100-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer100-conv"
    top: "layer100-conv"
    name: "layer100-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer100-conv"
    top: "layer100-conv"
    name: "layer100-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer100-conv"
    top: "layer101-conv"
    name: "layer101-conv"
    type: "Convolution"
    convolution_param {
        num_output: 256
        kernel_size: 3
        pad: 1
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer101-conv"
    top: "layer101-conv"
    name: "layer101-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer101-conv"
    top: "layer101-conv"
    name: "layer101-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer101-conv"
    top: "layer101-conv"
    name: "layer101-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer101-conv"
    top: "layer102-conv"
    name: "layer102-conv"
    type: "Convolution"
    convolution_param {
        num_output: 128
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer102-conv"
    top: "layer102-conv"
    name: "layer102-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer102-conv"
    top: "layer102-conv"
    name: "layer102-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer102-conv"
    top: "layer102-conv"
    name: "layer102-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer102-conv"
    top: "layer103-conv"
    name: "layer103-conv"
    type: "Convolution"
    convolution_param {
        num_output: 256
        kernel_size: 3
        pad: 1
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer103-conv"
    top: "layer103-conv"
    name: "layer103-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer103-conv"
    top: "layer103-conv"
    name: "layer103-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer103-conv"
    top: "layer103-conv"
    name: "layer103-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer103-conv"
    top: "layer104-conv"
    name: "layer104-conv"
    type: "Convolution"
    convolution_param {
        num_output: 128
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer104-conv"
    top: "layer104-conv"
    name: "layer104-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer104-conv"
    top: "layer104-conv"
    name: "layer104-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer104-conv"
    top: "layer104-conv"
    name: "layer104-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer104-conv"
    top: "layer105-conv"
    name: "layer105-conv"
    type: "Convolution"
    convolution_param {
        num_output: 256
        kernel_size: 3
        pad: 1
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer105-conv"
    top: "layer105-conv"
    name: "layer105-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer105-conv"
    top: "layer105-conv"
    name: "layer105-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer105-conv"
    top: "layer105-conv"
    name: "layer105-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer105-conv"
    top: "layer106-conv"
    name: "layer106-conv"
    type: "Convolution"
    convolution_param {
        num_output: 255
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: true
    }
}

layer {
  name: "slice1"
  type: "Slice"
  bottom: "layer82-conv"
  top: "slice1_1"
  top: "slice1_2"
  top: "slice1_3"
  slice_param {
      slice_dim: 1
      slice_point: 85
      slice_point: 170
  }
}

layer {
  name: "reshape1_1"
  type: "Reshape"
  bottom: "slice1_1"
  top: "reshape1_1"
  reshape_param {
    shape {
        dim: 0
        dim: 1
        dim: 85
        dim: -1
    }
  }
}

layer {
  name: "reshape1_2"
  type: "Reshape"
  bottom: "slice1_2"
  top: "reshape1_2"
  reshape_param {
    shape {
        dim: 0
        dim: 1
        dim: 85
        dim: -1
    }
  }
}

layer {
  name: "reshape1_3"
  type: "Reshape"
  bottom: "slice1_3"
  top: "reshape1_3"
  reshape_param {
    shape {
        dim: 0
        dim: 1
        dim: 85
        dim: -1
    }
  }
}

layer {
  name: "concat1"
  type: "Concat"
  bottom: "reshape1_1"
  bottom: "reshape1_2"
  bottom: "reshape1_3"
  top: "concat1"
  concat_param {
    axis: -1
  }
}

layer {
  name: "slice2"
  type: "Slice"
  bottom: "layer94-conv"
  top: "slice2_1"
  top: "slice2_2"
  top: "slice2_3"
  slice_param {
      slice_dim: 1
      slice_point: 85
      slice_point: 170
  }
}

layer {
  name: "reshape2_1"
  type: "Reshape"
  bottom: "slice2_1"
  top: "reshape2_1"
  reshape_param {
    shape {
        dim: 0
        dim: 1
        dim: 85
        dim: -1
    }
  }
}

layer {
  name: "reshape2_2"
  type: "Reshape"
  bottom: "slice2_2"
  top: "reshape2_2"
  reshape_param {
    shape {
        dim: 0
        dim: 1
        dim: 85
        dim: -1
    }
  }
}

layer {
  name: "reshape2_3"
  type: "Reshape"
  bottom: "slice2_3"
  top: "reshape2_3"
  reshape_param {
    shape {
        dim: 0
        dim: 1
        dim: 85
        dim: -1
    }
  }
}

layer {
  name: "concat2"
  type: "Concat"
  bottom: "reshape2_1"
  bottom: "reshape2_2"
  bottom: "reshape2_3"
  top: "concat2"
  concat_param {
    axis: -1
  }
}

layer {
  name: "slice3"
  type: "Slice"
  bottom: "layer106-conv"
  top: "slice3_1"
  top: "slice3_2"
  top: "slice3_3"
  slice_param {
      slice_dim: 1
      slice_point: 85
      slice_point: 170
  }
}

layer {
  name: "reshape3_1"
  type: "Reshape"
  bottom: "slice3_1"
  top: "reshape3_1"
  reshape_param {
    shape {
        dim: 0
        dim: 1
        dim: 85
        dim: -1
    }
  }
}

layer {
  name: "reshape3_2"
  type: "Reshape"
  bottom: "slice3_2"
  top: "reshape3_2"
  reshape_param {
    shape {
        dim: 0
        dim: 1
        dim: 85
        dim: -1
    }
  }
}

layer {
  name: "reshape3_3"
  type: "Reshape"
  bottom: "slice3_3"
  top: "reshape3_3"
  reshape_param {
    shape {
        dim: 0
        dim: 1
        dim: 85
        dim: -1
    }
  }
}

layer {
  name: "concat3"
  type: "Concat"
  bottom: "reshape3_1"
  bottom: "reshape3_2"
  bottom: "reshape3_3"
  top: "concat3"
  concat_param {
    axis: -1
  }
}

input: "data_bbox1"
input_shape {
  dim: 1
  dim: 1
  dim: 4
  dim: 507
}

input: "data_bbox2"
input_shape {
  dim: 1
  dim: 1
  dim: 4
  dim: 2028
}

input: "data_bbox3"
input_shape {
  dim: 1
  dim: 1
  dim: 4
  dim: 8112
}

layer {
  name: "bbox1_data_slice"
  type: "Slice"
  bottom: "data_bbox1"
  top: "colx1"
  top: "rowy1"
  top: "biasx1"
  top: "biasy1"
  slice_param {
      slice_dim: 2
      slice_point: 1
      slice_point: 2
      slice_point: 3
  }
}

layer {
  name: "bbox2_data_slice"
  type: "Slice"
  bottom: "data_bbox2"
  top: "colx2"
  top: "rowy2"
  top: "biasx2"
  top: "biasy2"
  slice_param {
      slice_dim: 2
      slice_point: 1
      slice_point: 2
      slice_point: 3
  }
}

layer {
  name: "bbox3_data_slice"
  type: "Slice"
  bottom: "data_bbox3"
  top: "colx3"
  top: "rowy3"
  top: "biasx3"
  top: "biasy3"
  slice_param {
      slice_dim: 2
      slice_point: 1
      slice_point: 2
      slice_point: 3
  }
}

layer {
  name: "concat1_slice"
  type: "Slice"
  bottom: "concat1"
  top: "x1"
  top: "y1"
  top: "w1"
  top: "h1"
  top: "objScore1"
  top: "clsScore1"
  slice_param {
      slice_dim: 2
      slice_point: 1
      slice_point: 2
      slice_point: 3
      slice_point: 4
      slice_point: 5
  }
}

layer {
  name: "concat2_slice"
  type: "Slice"
  bottom: "concat2"
  top: "x2"
  top: "y2"
  top: "w2"
  top: "h2"
  top: "objScore2"
  top: "clsScore2"
  slice_param {
      slice_dim: 2
      slice_point: 1
      slice_point: 2
      slice_point: 3
      slice_point: 4
      slice_point: 5
  }
}

layer {
  name: "concat3_slice"
  type: "Slice"
  bottom: "concat3"
  top: "x3"
  top: "y3"
  top: "w3"
  top: "h3"
  top: "objScore3"
  top: "clsScore3"
  slice_param {
      slice_dim: 2
      slice_point: 1
      slice_point: 2
      slice_point: 3
      slice_point: 4
      slice_point: 5
  }
}

##################################bbox1#####################################
# sgmx1 = sigmoid(x1)
layer {
  name: "sgmx1"
  type: "Sigmoid"
  bottom: "x1"
  top: "sgmx1"
}

# cx = sgmx1 + col
layer {
   name: "cx1"
   type: "Eltwise"
   bottom: "sgmx1"
   bottom: "colx1"
   top: "cx1"
   eltwise_param {
     operation: SUM
   }
}

# expw1 = exp(w1)
layer {
  name: "expw1"
  type: "Exp"
  bottom: "w1"
  top: "expw1"
}

# halfW1 = expw*bias
layer {
   name: "halfW1"
   type: "Eltwise"
   bottom: "expw1"
   bottom: "biasx1"
   top: "halfW1"
   eltwise_param {
     operation: PROD
   }
}

# xmin = (cx/gridW - 0.5*halfW)*imgH
layer {
   name: "xmin1"
   type: "Eltwise"
   bottom: "cx1"
   bottom: "halfW1"
   top: "xmin1"
   eltwise_param {
     operation: SUM
	 coeff: 32
     coeff: -0.5
   }
}

# xmax = (cx/gridW + 0.5*halfW)*imgH
layer {
   name: "xmax1"
   type: "Eltwise"
   bottom: "cx1"
   bottom: "halfW1"
   top: "xmax1"
   eltwise_param {
     operation: SUM
	 coeff: 32
     coeff: 0.5
   }
}

# sgmy1 = sigmoid(y1)
layer {
  name: "sgmy1"
  type: "Sigmoid"
  bottom: "y1"
  top: "sgmy1"
}

# cy = sgmy1 + col
layer {
   name: "cy1"
   type: "Eltwise"
   bottom: "sgmy1"
   bottom: "rowy1"
   top: "cy1"
   eltwise_param {
     operation: SUM
   }
}

# exph1 = exp(h1)
layer {
  name: "exph1"
  type: "Exp"
  bottom: "h1"
  top: "exph1"
}

# halfH1 = exph*bias
layer {
   name: "halfH1"
   type: "Eltwise"
   bottom: "exph1"
   bottom: "biasy1"
   top: "halfH1"
   eltwise_param {
     operation: PROD
   }
}

# ymin = halfH*coeff(0.5/gridNumWidth) - cy
layer {
   name: "ymin1"
   type: "Eltwise"
   bottom: "cy1"
   bottom: "halfH1"
   top: "ymin1"
   eltwise_param {
     operation: SUM
	 coeff: 32
     coeff: -0.5
   }
}

# ymax = halfH*coeff(0.5/gridNumWidth) + cy
layer {
   name: "ymax1"
   type: "Eltwise"
   bottom: "cy1"
   bottom: "halfH1"
   top: "ymax1"
   eltwise_param {
     operation: SUM
	 coeff: 32
     coeff: 0.5
   }
}

layer {
   name: "coordinate1"
   type: "Concat"
   bottom: "xmin1"
   bottom: "ymin1"
   bottom: "xmax1"
   bottom: "ymax1"
   top: "coordinate1"
   concat_param {
     concat_dim: 2
   }
}

# sgmClassScore = sigmoid(clsScore)
layer {
  name: "sgmClassScore1"
  type: "Sigmoid"
  bottom: "clsScore1"
  top: "sgmClassScore1"
}

# sgmObjScore = sigmoid(objScore)
layer {
  name: "sgmObjScore1"
  type: "Sigmoid"
  bottom: "objScore1"
  top: "sgmObjScore1"
}

layer {
	name: "maxClsScore1"
	type: "ArgMax"
	bottom: "sgmClassScore1"
	top: "maxClsScore1"
	argmax_param {
		out_max_val: 1
		top_k: 1
		axis:2
	}
}

layer {
	name: "argmaxIdx1"
	type: "ArgMax"
	bottom: "sgmClassScore1"
	top: "argmaxIdx1"
	argmax_param {
		top_k: 1
		axis:2
	}
}

# argmaxVal1 = sgmClassScore*sgmObjScore
layer {
   name: "argmaxVal1"
   type: "Eltwise"
   bottom: "maxClsScore1"
   bottom: "sgmObjScore1"
   top: "argmaxVal1"
   eltwise_param {
     operation: PROD
   }
}

##################################bbox2#####################################
# sgmx = sigmoid(x)
layer {
  name: "sgmx2"
  type: "Sigmoid"
  bottom: "x2"
  top: "sgmx2"
}

# cx = sgmx + col
layer {
   name: "cx2"
   type: "Eltwise"
   bottom: "sgmx2"
   bottom: "colx2"
   top: "cx2"
   eltwise_param {
     operation: SUM
   }
}

# expw1 = exp(w)
layer {
  name: "expw2"
  type: "Exp"
  bottom: "w2"
  top: "expw2"
}

# halfW = expw*bias
layer {
   name: "halfW2"
   type: "Eltwise"
   bottom: "expw2"
   bottom: "biasx2"
   top: "halfW2"
   eltwise_param {
     operation: PROD
   }
}

# xmin = halfW*coeff(0.5/gridNumWidth) - cx
layer {
   name: "xmin2"
   type: "Eltwise"
   bottom: "cx2"
   bottom: "halfW2"
   top: "xmin2"
   eltwise_param {
     operation: SUM
     coeff: 16
     coeff: -0.5
   }
}

# xmax = halfW*coeff(0.5/gridNumWidth) + cx
layer {
   name: "xmax2"
   type: "Eltwise"
   bottom: "cx2"
   bottom: "halfW2"
   top: "xmax2"
   eltwise_param {
     operation: SUM
     coeff: 16
     coeff: 0.5
   }
}

# sgmy1 = sigmoid(y)
layer {
  name: "sgmy2"
  type: "Sigmoid"
  bottom: "y2"
  top: "sgmy2"
}

# cy = sgmy + col
layer {
   name: "cy2"
   type: "Eltwise"
   bottom: "sgmy2"
   bottom: "rowy2"
   top: "cy2"
   eltwise_param {
     operation: SUM
   }
}

# exph = exp(h)
layer {
  name: "exph2"
  type: "Exp"
  bottom: "h2"
  top: "exph2"
}

# halfH = exph*bias
layer {
   name: "halfH2"
   type: "Eltwise"
   bottom: "exph2"
   bottom: "biasy2"
   top: "halfH2"
   eltwise_param {
     operation: PROD
   }
}

# ymin = halfH*coeff(0.5/gridNumWidth) - cy
layer {
   name: "ymin2"
   type: "Eltwise"
   bottom: "cy2"
   bottom: "halfH2"
   top: "ymin2"
   eltwise_param {
     operation: SUM
     coeff: 16
     coeff: -0.5
   }
}

# ymax = halfH*coeff(0.5/gridNumWidth) + cy
layer {
   name: "ymax2"
   type: "Eltwise"
   bottom: "cy2"
   bottom: "halfH2"
   top: "ymax2"
   eltwise_param {
     operation: SUM
     coeff: 16
     coeff: 0.5
   }
}

layer {
   name: "coordinate2"
   type: "Concat"
   bottom: "xmin2"
   bottom: "ymin2"
   bottom: "xmax2"
   bottom: "ymax2"
   top: "coordinate2"
   concat_param {
     concat_dim: 2
   }
}

# sgmClassScore = sigmoid(clsScore)
layer {
  name: "sgmClassScore2"
  type: "Sigmoid"
  bottom: "clsScore2"
  top: "sgmClassScore2"
}

# sgmObjScore = sigmoid(objScore)
layer {
  name: "sgmObjScore2"
  type: "Sigmoid"
  bottom: "objScore2"
  top: "sgmObjScore2"
}

layer {
	name: "maxClsScore2"
	type: "ArgMax"
	bottom: "sgmClassScore2"
	top: "maxClsScore2"
	argmax_param {
		out_max_val: 1
		top_k: 1
		axis:2
	}
}

layer {
	name: "argmaxIdx2"
	type: "ArgMax"
	bottom: "sgmClassScore2"
	top: "argmaxIdx2"
	argmax_param {
		top_k: 1
		axis:2
	}
}

# argmaxVal1 = sgmClassScore*sgmObjScore
layer {
   name: "argmaxVal2"
   type: "Eltwise"
   bottom: "maxClsScore2"
   bottom: "sgmObjScore2"
   top: "argmaxVal2"
   eltwise_param {
     operation: PROD
   }
}

##################################bbox3#####################################
# sgmx = sigmoid(x)
layer {
  name: "sgmx3"
  type: "Sigmoid"
  bottom: "x3"
  top: "sgmx3"
}

# cx = sgmx + col
layer {
   name: "cx3"
   type: "Eltwise"
   bottom: "sgmx3"
   bottom: "colx3"
   top: "cx3"
   eltwise_param {
     operation: SUM
   }
}

# expw1 = exp(w)
layer {
  name: "expw3"
  type: "Exp"
  bottom: "w3"
  top: "expw3"
}

# halfW = expw*bias
layer {
   name: "halfW3"
   type: "Eltwise"
   bottom: "expw3"
   bottom: "biasx3"
   top: "halfW3"
   eltwise_param {
     operation: PROD
   }
}

# xmin = halfW*coeff(0.5/gridNumWidth) - cx
layer {
   name: "xmin3"
   type: "Eltwise"
   bottom: "cx3"
   bottom: "halfW3"
   top: "xmin3"
   eltwise_param {
     operation: SUM
     coeff: 8
     coeff: -0.5
   }
}

# xmax = halfW*coeff(0.5/gridNumWidth) + cx
layer {
   name: "xmax3"
   type: "Eltwise"
   bottom: "cx3"
   bottom: "halfW3"
   top: "xmax3"
   eltwise_param {
     operation: SUM
     coeff: 8
     coeff: 0.5
   }
}

# sgmy = sigmoid(y)
layer {
  name: "sgmy3"
  type: "Sigmoid"
  bottom: "y3"
  top: "sgmy3"
}

# cy = sgmy + col  coeff=gridNumWidth,gridNumWidth
layer {
   name: "cy3"
   type: "Eltwise"
   bottom: "sgmy3"
   bottom: "rowy3"
   top: "cy3"
   eltwise_param {
     operation: SUM
   }
}

# exph = exp(h)
layer {
  name: "exph3"
  type: "Exp"
  bottom: "h3"
  top: "exph3"
}

# halfH = exph*bias
layer {
   name: "halfH3"
   type: "Eltwise"
   bottom: "exph3"
   bottom: "biasy3"
   top: "halfH3"
   eltwise_param {
     operation: PROD
   }
}

# ymin = halfH*coeff(0.5/gridNumWidth) - cy
layer {
   name: "ymin3"
   type: "Eltwise"
   bottom: "cy3"
   bottom: "halfH3"
   top: "ymin3"
   eltwise_param {
     operation: SUM
     coeff: 8
     coeff: -0.5
   }
}

# ymax = halfH*coeff(0.5/gridNumWidth) + cy
layer {
   name: "ymax3"
   type: "Eltwise"
   bottom: "cy3"
   bottom: "halfH3"
   top: "ymax3"
   eltwise_param {
     operation: SUM
     coeff: 8
     coeff: 0.5
   }
}

layer {
   name: "coordinate3"
   type: "Concat"
   bottom: "xmin3"
   bottom: "ymin3"
   bottom: "xmax3"
   bottom: "ymax3"
   top: "coordinate3"
   concat_param {
     concat_dim: 2
   }
}

# sgmClassScore = sigmoid(clsScore)
layer {
  name: "sgmClassScore3"
  type: "Sigmoid"
  bottom: "clsScore3"
  top: "sgmClassScore3"
}

# sgmObjScore = sigmoid(objScore)
layer {
  name: "sgmObjScore3"
  type: "Sigmoid"
  bottom: "objScore3"
  top: "sgmObjScore3"
}

layer {
	name: "maxClsScore3"
	type: "ArgMax"
	bottom: "sgmClassScore3"
	top: "maxClsScore3"
	argmax_param {
		out_max_val: 1
		top_k: 1
		axis:2
	}
}

layer {
	name: "argmaxIdx3"
	type: "ArgMax"
	bottom: "sgmClassScore3"
	top: "argmaxIdx3"
	argmax_param {
		top_k: 1
		axis:2
	}
}

# argmaxVal1 = sgmClassScore*sgmObjScore
layer {
   name: "argmaxVal3"
   type: "Eltwise"
   bottom: "maxClsScore3"
   bottom: "sgmObjScore3"
   top: "argmaxVal3"
   eltwise_param {
     operation: PROD
   }
}

layer {
   name: "coordinates"
   type: "Concat"
   bottom: "coordinate1"
   bottom: "coordinate2"
   bottom: "coordinate3"
   top: "coordinates"
   concat_param {
     concat_dim: 3
   }
}

layer {
   name: "argmaxIdxs"
   type: "Concat"
   bottom: "argmaxIdx1"
   bottom: "argmaxIdx2"
   bottom: "argmaxIdx3"
   top: "argmaxIdxs"
   concat_param {
     concat_dim: 3
   }
}

layer {
   name: "argmaxVals"
   type: "Concat"
   bottom: "argmaxVal1"
   bottom: "argmaxVal2"
   bottom: "argmaxVal3"
   top: "argmaxVals"
   concat_param {
     concat_dim: 3
   }
}

layer {
   name: "filterVal"
   type: "FilterVector"
   bottom: "argmaxVals"
   top: "filterVal"
   filter_vector_param {
       filter_thresh: 0.3
	   is_output_idx: false
	   is_report_vector_num:true
	   top_k:300
   }
}

layer {
   name: "filterIdx"
   type: "FilterVector"
   bottom: "argmaxVals"
   top: "filterIdx"
   filter_vector_param {
       filter_thresh: 0.3
	   is_output_idx: true
	   is_report_vector_num:false
	   top_k:300
   }
}